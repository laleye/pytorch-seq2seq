[encoder_model]
rnn_type = GRU
input_size = 256
hidden_size = 256
bidirectional = True
num_layers = 1
pretrained_embedding = False

[decoder_model]
rnn_type = GRU
with_attention = True
input_size = 256
# if encoder_model.bidirectional, hidden_size must be 2*encoder_model.hidden_size 
hidden_size = 512
drop_out=0.1
teacher_forcing_ratio=0.5
pretrained_embedding = False
